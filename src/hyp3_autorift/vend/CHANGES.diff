diff --git upstream/testGeogrid.py testGeogrid.py
--- testGeogrid.py
+++ testGeogrid.py
@@ -2,6 +2,7 @@
 
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # Copyright 2019 California Institute of Technology. ALL RIGHTS RESERVED.
+# Modifications Copyright 2021 Alaska Satellite Facility
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -224,7 +225,7 @@
     return info
 
 
-def coregisterLoadMetadata(indir_m, indir_s):
+def coregisterLoadMetadata(indir_m, indir_s, **kwargs):
     """
     Input file.
     """
@@ -256,6 +257,9 @@
     elif re.findall('LT0[45]_', DS.GetDescription()).__len__() > 0:
         nameString = os.path.basename(DS.GetDescription())
         info.time = nameString.split('_')[3]
+    elif 'sentinel-s2-l1c' in indir_m or 's2-l1c-us-west-2' in indir_m:
+        s2_name = kwargs['reference_metadata']['id']
+        info.time = s2_name.split('_')[2]
     elif re.findall('S2._', DS.GetDescription()).__len__() > 0:
         info.time = DS.GetDescription().split('_')[2]
     else:
@@ -279,6 +283,9 @@
     elif re.findall('LT0[45]_', DS1.GetDescription()).__len__() > 0:
         nameString1 = os.path.basename(DS1.GetDescription())
         info1.time = nameString1.split('_')[3]
+    elif 'sentinel-s2-l1c' in indir_s or 's2-l1c-us-west-2' in indir_s:
+        s2_name = kwargs['secondary_metadata']['id']
+        info1.time = s2_name.split('_')[2]
     elif re.findall('S2._', DS1.GetDescription()).__len__() > 0:
         info1.time = DS1.GetDescription().split('_')[2]
     else:
diff --git upstream/testautoRIFT.py testautoRIFT.py
--- testautoRIFT.py
+++ testautoRIFT.py
@@ -2,6 +2,7 @@
 
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # Copyright 2019 California Institute of Technology. ALL RIGHTS RESERVED.
+# Modifications Copyright 2021 Alaska Satellite Facility
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -299,6 +300,7 @@
     geogrid_run_info=None,
     preprocessing_methods=('hps', 'hps'),
     preprocessing_filter_width=5,
+    zero_mask=None,
 ):
     """
     Wire and run geogrid.
@@ -320,8 +322,8 @@
 
     # take the amplitude only for the radar images
     if optflag == 0:
-        I1 = np.abs(I1)
-        I2 = np.abs(I2)
+        I1 = loadProduct('reference.tif')
+        I2 = loadProduct('secondary.tif')
 
     obj.I1 = I1
     obj.I2 = I2
@@ -345,6 +347,7 @@
     #        We should not mask based on zero values in the L7 images as this percolates into SearchLimit{X,Y}
     #        and prevents autoRIFT from looking at large parts of the images, but untangling the logic here
     #        has proved too difficult, so lets just turn it off if `wallis_fill` preprocessing is going to be used.
+    #        However, we do have the image zero_mask already, so we can use that to create the output product noDataMask
     # generate the nodata mask where offset searching will be skipped based on 1) imported nodata mask and/or 2) zero values in the image
     if 'wallis_fill' not in preprocessing_methods:
         for ii in range(obj.xGrid.shape[0]):
@@ -354,6 +357,11 @@
                         I2[obj.yGrid[ii, jj] - 1, obj.xGrid[ii, jj] - 1] == 0
                     ):
                         noDataMask[ii, jj] = True
+    elif zero_mask is not None:
+        for ii in range(obj.xGrid.shape[0]):
+            for jj in range(obj.xGrid.shape[1]):
+                if (obj.yGrid[ii, jj] != nodata) & (obj.xGrid[ii, jj] != nodata):
+                    noDataMask[ii, jj] = zero_mask[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]
 
     # mask out nodata to skip the offset searching using the nodata mask (by setting SearchLimit to be 0)
 
@@ -426,24 +434,32 @@
     print('Pre-process Start!!!')
     print(f'Using Wallis Filter Width: {obj.WallisFilterWidth}')
 
-    # TODO: Allow different filters to be applied images independently
-    # default to most stringent filtering
+    # TODO: Allow different filters to be applied images independently; default to most stringent filtering
     if 'wallis_fill' in preprocessing_methods:
-        obj.preprocess_filt_wal_nodata_fill()
+        # FIXME: Ensuring landsat 7 images are projected correctly requires wallis_fill filtering and then reprojecting the
+        #        secondary scene before processing with Geogrid or autoRIFT; this now occurs in hyp3-autorift/process.py
+        warnings.warn('Wallis filtering must be done before processing with geogrid! Be careful when using this method',
+                      UserWarning)
+        obj.zeroMask = zero_mask
+        # obj.preprocess_filt_wal_nodata_fill()
     elif 'wallis' in preprocessing_methods:
-        obj.preprocess_filt_wal()
+        # FIXME: Ensuring landsat 7 images are projected correctly requires wallis filtering and then reprojecting the
+        #       secondary scene before processing with Geogrid or autoRIFT; this now occurs in hyp3-autorift/process.py
+        warnings.warn('Wallis filtering must be done before processing with geogrid! Be careful when using this method',
+                      UserWarning)
+        obj.zeroMask = zero_mask
+        # obj.preprocess_filt_wal()
     elif 'fft' in preprocessing_methods:
-        # FIXME: The Landsat 4/5 FFT preprocessor looks for the image corners to
-        #        determine the scene rotation, but Geogrid + autoRIFT rond the
-        #        corners when co-registering and chop the non-overlapping corners
-        #        when subsetting to the common image overlap. FFT filer needs to
-        #        be applied to the native images before they are processed by
-        #        Geogrid or autoRIFT.
+        # FIXME: Ensuring landsat 7 images are projected correctly requires fft filtering and then reprojecting the
+        #        secondary scene before processing with Geogrid or autoRIFT. Furthermore, the Landsat 4/5 FFT
+        #        preprocessor looks for the image corners to determine the scene rotation, but Geogrid + autoRIFT round
+        #        corners when co-registering and chop the non-overlapping corners when subsetting to the common image
+        #        overlap. FFT filer needs to  be applied to the native images before they are processed by Geogrid or
+        #        autoRIFT; this now occurs in hyp3-autorift/process.py
         # obj.preprocess_filt_wal()
         # obj.preprocess_filt_fft()
-        warnings.warn(
-            'FFT filtering must be done before processing with geogrid! Be careful when using this method', UserWarning
-        )
+        warnings.warn('FFT filtering must be done before processing with geogrid! Be careful when using this method',
+                      UserWarning)
     else:
         obj.preprocess_filt_hps()
     print('Pre-process Done!!!')
@@ -548,6 +564,7 @@
     mpflag,
     ncname,
     geogrid_run_info=None,
+    **kwargs,
 ):
     import numpy as np
     import time
@@ -636,7 +653,7 @@
     intermediate_nc_file = 'autoRIFT_intermediate.nc'
 
     if os.path.exists(intermediate_nc_file):
-        import netcdf_output as no
+        import hyp3_autorift.vend.netcdf_output as no
 
         (
             Dx,
@@ -656,7 +673,7 @@
 
         # FIXME: Filter width is a magic variable here and not exposed well.
         preprocessing_filter_width = 5
-        if nc_sensor == 'S1':
+        if nc_sensor == 'S1' or nc_sensor == 'GS1':  # GS1?
             preprocessing_filter_width = 21
 
         print(f'Preprocessing filter width {preprocessing_filter_width}')
@@ -664,12 +681,21 @@
         preprocessing_methods = ['hps', 'hps']
         for ii, name in enumerate((m_name, s_name)):
             if len(re.findall('L[EO]07_', name)) > 0:
-                acquisition = datetime.strptime(name.split('_')[3], '%Y%m%d')
-                if acquisition >= datetime(2003, 5, 31):
-                    preprocessing_methods[ii] = 'wallis_fill'
+                preprocessing_methods[ii] = 'wallis_fill'
             elif len(re.findall('LT0[45]_', name)) > 0:
                 preprocessing_methods[ii] = 'fft'
 
+        zero_mask = None
+        indir_m_zero = f'{indir_m.split(".")[0]}_zeroMask.{indir_m.split(".")[1]}'
+        indir_s_zero = f'{indir_s.split(".")[0]}_zeroMask.{indir_s.split(".")[1]}'
+        if os.path.exists(indir_m_zero) or os.path.exists(indir_s_zero):
+            m_zero, s_zero = loadProductOptical(indir_m_zero, indir_s_zero)
+            m_zero = m_zero.astype(np.uint8)
+            s_zero = s_zero.astype(np.uint8)
+
+            zero_mask = m_zero | s_zero
+            zero_mask = zero_mask.astype(np.uint8)
+
         print(f'Using preprocessing methods {preprocessing_methods}')
 
         (
@@ -703,9 +729,10 @@
             geogrid_run_info=geogrid_run_info,
             preprocessing_methods=preprocessing_methods,
             preprocessing_filter_width=preprocessing_filter_width,
+            zero_mask=zero_mask,
         )
         if nc_sensor is not None:
-            import netcdf_output as no
+            import hyp3_autorift.vend.netcdf_output as no
 
             no.netCDF_packaging_intermediate(
                 Dx,
@@ -754,9 +781,9 @@
     if SSM is not None:
         SSM[SEARCHLIMITX == 0] = False
 
-    import scipy.io as sio
-
-    sio.savemat('offset.mat', {'Dx': DX, 'Dy': DY, 'InterpMask': INTERPMASK, 'ChipSizeX': CHIPSIZEX})
+    # import scipy.io as sio
+    #
+    # sio.savemat('offset.mat', {'Dx': DX, 'Dy': DY, 'InterpMask': INTERPMASK, 'ChipSizeX': CHIPSIZEX})
 
     netcdf_file = None
     if grid_location is not None:
@@ -849,7 +876,7 @@
             if nc_sensor is not None:
                 if nc_sensor == 'S1':
                     swath_offset_bias_ref = [-0.01, 0.019, -0.0068, 0.006]
-                    import netcdf_output as no
+                    import hyp3_autorift.vend.netcdf_output as no
 
                     DX, DY, flight_direction_m, flight_direction_s = no.cal_swath_offset_bias(
                         indir_m,
@@ -1016,7 +1043,7 @@
                     master_split = str.split(master_filename, '_')
                     slave_split = str.split(slave_filename, '_')
 
-                    import netcdf_output as no
+                    import hyp3_autorift.vend.netcdf_output as no
 
                     pair_type = 'radar'
                     detection_method = 'feature'
@@ -1029,10 +1056,16 @@
                         raise Exception('Input search range is all zero everywhere, thus no search conducted')
                     PPP = roi_valid_percentage * 100
                     if ncname is None:
-                        out_nc_filename = (
-                            f'./{master_filename[0:-4]}_X_{slave_filename[0:-4]}'
-                            f'_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
-                        )
+                        if '.zip' in master_filename:
+                            out_nc_filename = (
+                                f'./{master_filename[0:-4]}_X_{slave_filename[0:-4]}'
+                                f'_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
+                            )
+                        elif '.SAFE' in master_filename:
+                            out_nc_filename = (
+                                f'./{master_filename[0:-5]}_X_{slave_filename[0:-5]}'
+                                f'_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
+                            )
                     else:
                         out_nc_filename = f'{ncname}_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
                     CHIPSIZEY = np.round(CHIPSIZEX * ScaleChipSizeY / 2) * 2
@@ -1126,6 +1159,7 @@
                         dx_mean_shift1,
                         dy_mean_shift1,
                         error_vector,
+                        parameter_file=kwargs['parameter_file'],
                     )
 
                 elif nc_sensor in ('L4', 'L5', 'L7', 'L8', 'L9'):
@@ -1153,7 +1187,7 @@
                     master_split = str.split(master_filename, '_')
                     slave_split = str.split(slave_filename, '_')
 
-                    import netcdf_output as no
+                    import hyp3_autorift.vend.netcdf_output as no
 
                     pair_type = 'optical'
                     detection_method = 'feature'
@@ -1174,8 +1208,8 @@
                         out_nc_filename = f'{ncname}_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
                     CHIPSIZEY = np.round(CHIPSIZEX * ScaleChipSizeY / 2) * 2
 
-                    d0 = datetime(int(master_split[3][0:4]), int(master_split[3][4:6]), int(master_split[3][6:8]))
-                    d1 = datetime(int(slave_split[3][0:4]), int(slave_split[3][4:6]), int(slave_split[3][6:8]))
+                    d0 = datetime.strptime(kwargs['reference_metadata']['properties']['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ')
+                    d1 = datetime.strptime(kwargs['secondary_metadata']['properties']['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ')
                     date_dt_base = (d1 - d0).total_seconds() / timedelta(days=1).total_seconds()
                     date_dt = np.float64(date_dt_base)
                     if date_dt < 0:
@@ -1266,6 +1300,7 @@
                         dx_mean_shift1,
                         dy_mean_shift1,
                         error_vector,
+                        parameter_file=kwargs['parameter_file'],
                     )
 
                 elif nc_sensor == 'S2':
@@ -1284,26 +1319,13 @@
                         YPixelSize = geogrid_run_info['YPixelSize']
                         epsg = geogrid_run_info['epsg']
 
-                    master_path = indir_m
-                    slave_path = indir_s
-
-                    master_split = master_path.split('_')
-                    slave_split = slave_path.split('_')
+                    master_id = kwargs['reference_metadata']['id']
+                    slave_id = kwargs['secondary_metadata']['id']
 
-                    if re.findall('://', master_path).__len__() > 0:
-                        master_filename_full = master_path.split('/')
-                        for item in master_filename_full:
-                            if re.findall('S2._', item).__len__() > 0:
-                                master_filename = item
-                        slave_filename_full = slave_path.split('/')
-                        for item in slave_filename_full:
-                            if re.findall('S2._', item).__len__() > 0:
-                                slave_filename = item
-                    else:
-                        master_filename = os.path.basename(master_path)[:-8]
-                        slave_filename = os.path.basename(slave_path)[:-8]
+                    master_split = master_id.split('_')
+                    slave_split = slave_id.split('_')
 
-                    import netcdf_output as no
+                    import hyp3_autorift.vend.netcdf_output as no
 
                     pair_type = 'optical'
                     detection_method = 'feature'
@@ -1317,14 +1339,14 @@
                     PPP = roi_valid_percentage * 100
                     if ncname is None:
                         out_nc_filename = (
-                            f'./{master_filename}_X_{slave_filename}_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
+                            f'./{master_id}_X_{slave_id}_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
                         )
                     else:
                         out_nc_filename = f'{ncname}_G{gridspacingx:04.0f}V02_P{np.floor(PPP):03.0f}.nc'
                     CHIPSIZEY = np.round(CHIPSIZEX * ScaleChipSizeY / 2) * 2
 
-                    d0 = datetime(int(master_split[2][0:4]), int(master_split[2][4:6]), int(master_split[2][6:8]))
-                    d1 = datetime(int(slave_split[2][0:4]), int(slave_split[2][4:6]), int(slave_split[2][6:8]))
+                    d0 = datetime.strptime(kwargs['reference_metadata']['properties']['datetime'], '%Y-%m-%dT%H:%M:%SZ')
+                    d1 = datetime.strptime(kwargs['secondary_metadata']['properties']['datetime'], '%Y-%m-%dT%H:%M:%SZ')
                     date_dt_base = (d1 - d0).total_seconds() / timedelta(days=1).total_seconds()
                     date_dt = np.float64(date_dt_base)
                     if date_dt < 0:
@@ -1337,12 +1359,12 @@
                     slave_dt = d1.strftime('%Y%m%dT%H:%M:%S.%f').rstrip('0')
 
                     IMG_INFO_DICT = {
-                        'id_img1': master_filename,
-                        'id_img2': slave_filename,
+                        'id_img1': master_id,
+                        'id_img2': slave_id,
                         'acquisition_date_img1': master_dt,
                         'acquisition_date_img2': slave_dt,
-                        'correction_level_img1': master_split[4][:3],
-                        'correction_level_img2': slave_split[4][:3],
+                        'correction_level_img1': master_split[1][3:],
+                        'correction_level_img2': slave_split[1][3:],
                         'mission_img1': master_split[0][-3],
                         'mission_img2': slave_split[0][-3],
                         'satellite_img1': master_split[0][-2:],
@@ -1405,6 +1427,7 @@
                         dx_mean_shift1,
                         dy_mean_shift1,
                         error_vector,
+                        parameter_file=kwargs['parameter_file'],
                     )
 
                 elif nc_sensor is None:
diff --git upstream/netcdf_output.py netcdf_output.py
--- netcdf_output.py
+++ netcdf_output.py
@@ -1,6 +1,7 @@
-#!/usr/bin/env python3
 # Yang Lei, Jet Propulsion Laboratory
 # November 2017
+# Modifications Copyright 2021 Alaska Satellite Facility
+
 import datetime
 import os
 
@@ -8,6 +9,8 @@
 import numpy as np
 import pandas as pd
 
+import hyp3_autorift
+
 
 def get_satellite_attribute(info):
     mission_mapping = {
@@ -260,6 +263,7 @@
     dx_mean_shift1,
     dy_mean_shift1,
     error_vector,
+    parameter_file,
 ):
     vx_mean_shift = offset2vx_1 * dx_mean_shift + offset2vx_2 * dy_mean_shift
     temp = vx_mean_shift
@@ -418,9 +422,9 @@
     author = 'Alex S. Gardner, JPL/NASA; Yang Lei, GPS/Caltech'
     institution = 'NASA Jet Propulsion Laboratory (JPL), California Institute of Technology'
 
-    source = (
-        f'NASA MEaSUREs ITS_LIVE project. Processed with autoRIFT version {IMG_INFO_DICT["autoRIFT_software_version"]}'
-    )
+    source = f'NASA MEaSUREs ITS_LIVE project. Processed by ASF DAAC HyP3 {datetime.datetime.now().year} using the ' \
+             f'{hyp3_autorift.__name__} plugin version {hyp3_autorift.__version__} running autoRIFT version ' \
+             f'{IMG_INFO_DICT["autoRIFT_software_version"]}'
     if pair_type == 'radar':
         import isce3
 
@@ -439,8 +443,10 @@
         '  and Its Application for Tracking Ice Displacement. Remote Sensing, 13(4), p.749.\n'
         '  https://doi.org/10.3390/rs13040749\n'
         '\n'
-        'Additionally, a DOI is provided for the software used to generate this data:\n'
+        'Additionally, DOI\'s are provided for the software used to generate this data:\n'
         '* autoRIFT: https://doi.org/10.5281/zenodo.4025445\n'
+        '* HyP3 autoRIFT plugin: https://doi.org/10.5281/zenodo.4037016\n'
+        '* HyP3 processing environment: https://doi.org/10.5281/zenodo.3962581'
     )
     tran = [tran[0] + tran[1] / 2, tran[1], 0.0, tran[3] + tran[5] / 2, 0.0, tran[5]]
 
@@ -454,6 +460,7 @@
     nc_outfile.setncattr('date_created', datetime.datetime.now().strftime('%d-%b-%Y %H:%M:%S'))
     nc_outfile.setncattr('title', title)
     nc_outfile.setncattr('autoRIFT_software_version', IMG_INFO_DICT['autoRIFT_software_version'])
+    nc_outfile.setncattr('autoRIFT_parameter_file', parameter_file)
     nc_outfile.setncattr('scene_pair_type', pair_type)
     nc_outfile.setncattr('satellite', get_satellite_attribute(IMG_INFO_DICT))
     nc_outfile.setncattr('motion_detection_method', detection_method)
